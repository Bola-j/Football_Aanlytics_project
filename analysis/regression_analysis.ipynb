{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb42dff",
   "metadata": {},
   "source": [
    "# Comprehensive Regression Analysis for Football Analytics Project\n",
    "\n",
    "This notebook implements regression models for all three research questions:\n",
    "1. **Question 1**: Possession vs Match Outcomes (Logistic Regression)\n",
    "2. **Question 2**: Salary vs Performance (Multiple Linear Regression)\n",
    "3. **Question 3**: Age vs Performance (Linear and Polynomial Regression)\n",
    "\n",
    "Each section includes:\n",
    "- Model building and training\n",
    "- Model evaluation (R², RMSE, MAE, classification metrics)\n",
    "- Visualization of predictions vs actual\n",
    "- Feature importance/coefficients analysis\n",
    "- Statistical significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baca262",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67197119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,\n",
    "    classification_report, roc_auc_score, roc_curve,\n",
    "    r2_score, mean_squared_error, mean_absolute_error\n",
    ")\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b13683",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 1: Possession vs Match Outcomes - Logistic Regression\n",
    "\n",
    "**Research Question**: Can possession percentage predict match results?\n",
    "\n",
    "**Approach**: \n",
    "- Multinomial Logistic Regression (3 classes: Win, Draw, Loss)\n",
    "- Binary Logistic Regression (Win vs Not-Win)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9bc9d0",
   "metadata": {},
   "source": [
    "## 1.1 Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load possession data\n",
    "possession_df = pd.read_csv('../data_raw/PL_LaLiga_possession_2023_2024.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(\"Dataset Shape:\", possession_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(possession_df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(possession_df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(possession_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Feature: Possession percentage\n",
    "X_poss = possession_df[['Poss']].values\n",
    "\n",
    "# Target: Match result\n",
    "# Map results to numeric: W=2, D=1, L=0\n",
    "result_mapping = {'W': 2, 'D': 1, 'L': 0}\n",
    "y_poss_multi = possession_df['Result'].map(result_mapping).values\n",
    "\n",
    "# Binary target: Win vs Not-Win\n",
    "y_poss_binary = (possession_df['Result'] == 'W').astype(int).values\n",
    "\n",
    "print(\"Feature shape:\", X_poss.shape)\n",
    "print(\"Target (multiclass) shape:\", y_poss_multi.shape)\n",
    "print(\"Target (binary) shape:\", y_poss_binary.shape)\n",
    "print(\"\\nClass distribution (multiclass):\")\n",
    "print(pd.Series(y_poss_multi).value_counts().sort_index())\n",
    "print(\"\\nClass distribution (binary):\")\n",
    "print(pd.Series(y_poss_binary).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d43aec",
   "metadata": {},
   "source": [
    "## 1.2 Binary Logistic Regression: Win vs Not-Win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96533db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_poss, X_test_poss, y_train_binary, y_test_binary = train_test_split(\n",
    "    X_poss, y_poss_binary, test_size=0.2, random_state=42, stratify=y_poss_binary\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_poss = StandardScaler()\n",
    "X_train_poss_scaled = scaler_poss.fit_transform(X_train_poss)\n",
    "X_test_poss_scaled = scaler_poss.transform(X_test_poss)\n",
    "\n",
    "# Train logistic regression model\n",
    "log_reg_binary = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg_binary.fit(X_train_poss_scaled, y_train_binary)\n",
    "\n",
    "# Predictions\n",
    "y_pred_binary = log_reg_binary.predict(X_test_poss_scaled)\n",
    "y_pred_proba_binary = log_reg_binary.predict_proba(X_test_poss_scaled)[:, 1]\n",
    "\n",
    "print(\"Binary Logistic Regression Model Trained!\")\n",
    "print(f\"\\nCoefficient (Possession): {log_reg_binary.coef_[0][0]:.4f}\")\n",
    "print(f\"Intercept: {log_reg_binary.intercept_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate binary model\n",
    "print(\"=\" * 60)\n",
    "print(\"BINARY LOGISTIC REGRESSION RESULTS (Win vs Not-Win)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Accuracy metrics\n",
    "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
    "precision = precision_score(y_test_binary, y_pred_binary)\n",
    "recall = recall_score(y_test_binary, y_pred_binary)\n",
    "f1 = f1_score(y_test_binary, y_pred_binary)\n",
    "\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# ROC-AUC\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test_binary, y_pred_proba_binary)\n",
    "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "except:\n",
    "    print(\"ROC-AUC:   Could not calculate\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_binary = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "print(cm_binary)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred_binary, \n",
    "                          target_names=['Not-Win', 'Win']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eece0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize binary model results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "sns.heatmap(cm_binary, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Not-Win', 'Win'], yticklabels=['Not-Win', 'Win'])\n",
    "axes[0].set_title('Confusion Matrix - Win vs Not-Win', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "\n",
    "# 2. ROC Curve\n",
    "try:\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binary, y_pred_proba_binary)\n",
    "    axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "    axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
    "    axes[1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "except:\n",
    "    axes[1].text(0.5, 0.5, 'ROC Curve\\nNot Available', \n",
    "                ha='center', va='center', fontsize=12)\n",
    "\n",
    "# 3. Possession Distribution by Actual Result\n",
    "test_results_df = pd.DataFrame({\n",
    "    'Possession': X_test_poss.flatten(),\n",
    "    'Actual': ['Win' if y == 1 else 'Not-Win' for y in y_test_binary],\n",
    "    'Predicted': ['Win' if y == 1 else 'Not-Win' for y in y_pred_binary]\n",
    "})\n",
    "\n",
    "sns.boxplot(data=test_results_df, x='Actual', y='Possession', ax=axes[2])\n",
    "axes[2].set_title('Possession Distribution by Result', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Match Result', fontsize=12)\n",
    "axes[2].set_ylabel('Possession %', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nPossession Statistics by Actual Result:\")\n",
    "print(test_results_df.groupby('Actual')['Possession'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e685ac",
   "metadata": {},
   "source": [
    "## 1.3 Multinomial Logistic Regression: Win vs Draw vs Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for multiclass\n",
    "X_train_poss_m, X_test_poss_m, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_poss, y_poss_multi, test_size=0.2, random_state=42, stratify=y_poss_multi\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler_poss_m = StandardScaler()\n",
    "X_train_poss_m_scaled = scaler_poss_m.fit_transform(X_train_poss_m)\n",
    "X_test_poss_m_scaled = scaler_poss_m.transform(X_test_poss_m)\n",
    "\n",
    "# Train multinomial logistic regression\n",
    "log_reg_multi = LogisticRegression(multi_class='multinomial', solver='lbfgs', \n",
    "                                   random_state=42, max_iter=1000)\n",
    "log_reg_multi.fit(X_train_poss_m_scaled, y_train_multi)\n",
    "\n",
    "# Predictions\n",
    "y_pred_multi = log_reg_multi.predict(X_test_poss_m_scaled)\n",
    "y_pred_proba_multi = log_reg_multi.predict_proba(X_test_poss_m_scaled)\n",
    "\n",
    "print(\"Multinomial Logistic Regression Model Trained!\")\n",
    "print(f\"\\nCoefficients for each class:\")\n",
    "for i, class_name in enumerate(['Loss', 'Draw', 'Win']):\n",
    "    print(f\"  {class_name}: {log_reg_multi.coef_[i][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14354a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate multiclass model\n",
    "print(\"=\" * 60)\n",
    "print(\"MULTINOMIAL LOGISTIC REGRESSION RESULTS (Win vs Draw vs Loss)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_multi = accuracy_score(y_test_multi, y_pred_multi)\n",
    "print(f\"\\nOverall Accuracy: {accuracy_multi:.4f} ({accuracy_multi*100:.2f}%)\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_multi = confusion_matrix(y_test_multi, y_pred_multi)\n",
    "print(cm_multi)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test_multi, y_pred_multi, \n",
    "                          target_names=['Loss', 'Draw', 'Win']))\n",
    "\n",
    "# Weighted metrics\n",
    "precision_weighted = precision_score(y_test_multi, y_pred_multi, average='weighted')\n",
    "recall_weighted = recall_score(y_test_multi, y_pred_multi, average='weighted')\n",
    "f1_weighted = f1_score(y_test_multi, y_pred_multi, average='weighted')\n",
    "\n",
    "print(f\"\\nWeighted Metrics:\")\n",
    "print(f\"  Precision: {precision_weighted:.4f}\")\n",
    "print(f\"  Recall:    {recall_weighted:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3028d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiclass results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='YlOrRd', ax=axes[0],\n",
    "            xticklabels=['Loss', 'Draw', 'Win'], \n",
    "            yticklabels=['Loss', 'Draw', 'Win'])\n",
    "axes[0].set_title('Confusion Matrix - Multiclass', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Actual Result', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Result', fontsize=12)\n",
    "\n",
    "# 2. Possession by Result\n",
    "test_multi_df = pd.DataFrame({\n",
    "    'Possession': X_test_poss_m.flatten(),\n",
    "    'Actual': ['Loss' if y == 0 else 'Draw' if y == 1 else 'Win' for y in y_test_multi]\n",
    "})\n",
    "\n",
    "sns.violinplot(data=test_multi_df, x='Actual', y='Possession', ax=axes[1],\n",
    "              order=['Loss', 'Draw', 'Win'])\n",
    "axes[1].set_title('Possession Distribution by Match Result', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Match Result', fontsize=12)\n",
    "axes[1].set_ylabel('Possession %', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mean possession by result\n",
    "print(\"\\nMean Possession by Result:\")\n",
    "print(test_multi_df.groupby('Actual')['Possession'].agg(['mean', 'std', 'count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207287c7",
   "metadata": {},
   "source": [
    "## 1.4 Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69360f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use statsmodels for detailed statistics\n",
    "X_poss_with_const = sm.add_constant(X_poss)\n",
    "\n",
    "# Fit logistic regression using statsmodels for p-values\n",
    "logit_model = sm.Logit(y_poss_binary, X_poss_with_const)\n",
    "logit_result = logit_model.fit()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL SIGNIFICANCE TEST (Statsmodels)\")\n",
    "print(\"=\" * 60)\n",
    "print(logit_result.summary())\n",
    "\n",
    "# Extract key statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Coefficient for Possession: {logit_result.params[1]:.6f}\")\n",
    "print(f\"P-value: {logit_result.pvalues[1]:.6f}\")\n",
    "print(f\"Odds Ratio: {np.exp(logit_result.params[1]):.6f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if logit_result.pvalues[1] < 0.05:\n",
    "    print(\"  ✓ Possession is STATISTICALLY SIGNIFICANT (p < 0.05)\")\n",
    "    print(f\"  ✓ For every 1% increase in possession, odds of winning increase by {(np.exp(logit_result.params[1])-1)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"  ✗ Possession is NOT statistically significant (p >= 0.05)\")\n",
    "    print(\"  ✗ Cannot confidently conclude possession predicts match outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb25575",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 2: Salary vs Performance - Multiple Linear Regression\n",
    "\n",
    "**Research Question**: Do performance metrics predict player salaries?\n",
    "\n",
    "**Approach**: Multiple Linear Regression with performance metrics as predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdadb8a0",
   "metadata": {},
   "source": [
    "## 2.1 Load and Prepare Salary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce873459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load salary data\n",
    "salary_df = pd.read_csv('../data_raw/player_stats_with_salaries.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", salary_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(salary_df.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(salary_df.columns.tolist())\n",
    "print(\"\\nMissing values:\")\n",
    "print(salary_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf6ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare salary data\n",
    "# Select relevant columns for regression\n",
    "performance_cols = ['Gls', 'Ast', 'xG', 'npxG', 'xAG', 'Min', 'MP']\n",
    "target_col = 'Annual Gross'\n",
    "\n",
    "# Check which columns exist\n",
    "available_perf_cols = [col for col in performance_cols if col in salary_df.columns]\n",
    "print(f\"Available performance columns: {available_perf_cols}\")\n",
    "\n",
    "# Check target column\n",
    "if target_col in salary_df.columns:\n",
    "    print(f\"Target column '{target_col}' found\")\n",
    "else:\n",
    "    # Try alternative column names\n",
    "    salary_cols = [col for col in salary_df.columns if 'salary' in col.lower() or 'annual' in col.lower()]\n",
    "    print(f\"Alternative salary columns: {salary_cols}\")\n",
    "    if salary_cols:\n",
    "        target_col = salary_cols[0]\n",
    "        print(f\"Using '{target_col}' as target\")\n",
    "\n",
    "# Create clean dataset\n",
    "regression_cols = available_perf_cols + [target_col]\n",
    "salary_clean = salary_df[regression_cols].copy()\n",
    "\n",
    "# Remove rows with missing values\n",
    "salary_clean = salary_clean.dropna()\n",
    "\n",
    "print(f\"\\nClean dataset shape: {salary_clean.shape}\")\n",
    "print(f\"Rows dropped due to missing values: {len(salary_df) - len(salary_clean)}\")\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "print(salary_clean.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and handle outliers using IQR method\n",
    "def remove_outliers_iqr(df, columns, multiplier=1.5):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "        \n",
    "        before = len(df_clean)\n",
    "        df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "        after = len(df_clean)\n",
    "        removed = before - after\n",
    "        if removed > 0:\n",
    "            print(f\"{col}: Removed {removed} outliers ({removed/before*100:.1f}%)\")\n",
    "    return df_clean\n",
    "\n",
    "# Optional: Remove extreme outliers\n",
    "print(\"Outlier analysis:\")\n",
    "salary_no_outliers = remove_outliers_iqr(salary_clean, [target_col], multiplier=3.0)\n",
    "print(f\"\\nDataset after outlier removal: {salary_no_outliers.shape}\")\n",
    "\n",
    "# Use dataset without extreme outliers\n",
    "salary_final = salary_no_outliers.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3bb54",
   "metadata": {},
   "source": [
    "## 2.2 Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_salary = salary_final[available_perf_cols].values\n",
    "y_salary = salary_final[target_col].values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_sal, X_test_sal, y_train_sal, y_test_sal = train_test_split(\n",
    "    X_salary, y_salary, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_salary = StandardScaler()\n",
    "X_train_sal_scaled = scaler_salary.fit_transform(X_train_sal)\n",
    "X_test_sal_scaled = scaler_salary.transform(X_test_sal)\n",
    "\n",
    "print(f\"Training set: {X_train_sal_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_sal_scaled.shape}\")\n",
    "print(f\"\\nFeature names: {available_perf_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99aaafd",
   "metadata": {},
   "source": [
    "## 2.3 Check for Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a28e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = salary_final[available_perf_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Variance Inflation Factor (VIF)\n",
    "print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "print(\"VIF > 10 indicates high multicollinearity\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = available_perf_cols\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_sal_scaled, i) \n",
    "                   for i in range(len(available_perf_cols))]\n",
    "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "print(vif_data.to_string(index=False))\n",
    "\n",
    "# Identify problematic features\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]\n",
    "if len(high_vif) > 0:\n",
    "    print(\"\\n⚠️  High multicollinearity detected in:\")\n",
    "    print(high_vif.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n✓ No severe multicollinearity detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e0ae83",
   "metadata": {},
   "source": [
    "## 2.4 Train Multiple Linear Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ordinary Least Squares (OLS) Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_sal_scaled, y_train_sal)\n",
    "y_pred_lr = lr_model.predict(X_test_sal_scaled)\n",
    "\n",
    "# 2. Ridge Regression (L2 regularization)\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train_sal_scaled, y_train_sal)\n",
    "y_pred_ridge = ridge_model.predict(X_test_sal_scaled)\n",
    "\n",
    "# 3. Lasso Regression (L1 regularization)\n",
    "lasso_model = Lasso(alpha=1.0, random_state=42, max_iter=10000)\n",
    "lasso_model.fit(X_train_sal_scaled, y_train_sal)\n",
    "y_pred_lasso = lasso_model.predict(X_test_sal_scaled)\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de998f6f",
   "metadata": {},
   "source": [
    "## 2.5 Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each model\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'R²': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE (%)': mape\n",
    "    }\n",
    "\n",
    "# Compare models\n",
    "models_comparison = pd.DataFrame([\n",
    "    calculate_metrics(y_test_sal, y_pred_lr, 'Linear Regression'),\n",
    "    calculate_metrics(y_test_sal, y_pred_ridge, 'Ridge Regression'),\n",
    "    calculate_metrics(y_test_sal, y_pred_lasso, 'Lasso Regression')\n",
    "])\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON - SALARY PREDICTION\")\n",
    "print(\"=\" * 70)\n",
    "print(models_comparison.to_string(index=False))\n",
    "print(\"\\nBest model by R²:\", models_comparison.loc[models_comparison['R²'].idxmax(), 'Model'])\n",
    "print(\"Best model by RMSE:\", models_comparison.loc[models_comparison['RMSE'].idxmin(), 'Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e79c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for robust evaluation\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"5-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': lr_model,\n",
    "    'Ridge Regression': ridge_model,\n",
    "    'Lasso Regression': lasso_model\n",
    "}\n",
    "\n",
    "cv_results = []\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_sal_scaled, y_train_sal, \n",
    "                            cv=5, scoring='r2')\n",
    "    cv_results.append({\n",
    "        'Model': name,\n",
    "        'Mean R²': scores.mean(),\n",
    "        'Std R²': scores.std(),\n",
    "        'Min R²': scores.min(),\n",
    "        'Max R²': scores.max()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "print(cv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b0143",
   "metadata": {},
   "source": [
    "## 2.6 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and compare coefficients\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': available_perf_cols,\n",
    "    'Linear Reg': lr_model.coef_,\n",
    "    'Ridge': ridge_model.coef_,\n",
    "    'Lasso': lasso_model.coef_\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE COEFFICIENTS (Standardized)\")\n",
    "print(\"=\" * 70)\n",
    "print(coef_comparison.to_string(index=False))\n",
    "\n",
    "# Visualize coefficients\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (model_name, col_name) in enumerate([('Linear Regression', 'Linear Reg'),\n",
    "                                                ('Ridge Regression', 'Ridge'),\n",
    "                                                ('Lasso Regression', 'Lasso')]):\n",
    "    coef_df = coef_comparison[['Feature', col_name]].sort_values(col_name, ascending=False)\n",
    "    \n",
    "    axes[idx].barh(coef_df['Feature'], coef_df[col_name], \n",
    "                   color=['green' if x > 0 else 'red' for x in coef_df[col_name]])\n",
    "    axes[idx].set_xlabel('Coefficient Value', fontsize=11)\n",
    "    axes[idx].set_title(f'{model_name}\\nFeature Importance', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify most important features\n",
    "abs_coefs = coef_comparison.copy()\n",
    "abs_coefs['Avg_Abs_Coef'] = abs_coefs[['Linear Reg', 'Ridge', 'Lasso']].abs().mean(axis=1)\n",
    "abs_coefs = abs_coefs.sort_values('Avg_Abs_Coef', ascending=False)\n",
    "\n",
    "print(\"\\nMost Important Features (by average absolute coefficient):\")\n",
    "print(abs_coefs[['Feature', 'Avg_Abs_Coef']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e20f0c",
   "metadata": {},
   "source": [
    "## 2.7 Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "predictions = [\n",
    "    ('Linear Regression', y_pred_lr),\n",
    "    ('Ridge Regression', y_pred_ridge),\n",
    "    ('Lasso Regression', y_pred_lasso)\n",
    "]\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(predictions):\n",
    "    # Scatter plot\n",
    "    axes[idx].scatter(y_test_sal, y_pred, alpha=0.6, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test_sal.min(), y_pred.min())\n",
    "    max_val = max(y_test_sal.max(), y_pred.max())\n",
    "    axes[idx].plot([min_val, max_val], [min_val, max_val], \n",
    "                   'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[idx].set_xlabel('Actual Salary', fontsize=11)\n",
    "    axes[idx].set_ylabel('Predicted Salary', fontsize=11)\n",
    "    \n",
    "    r2 = r2_score(y_test_sal, y_pred)\n",
    "    axes[idx].set_title(f'{name}\\nR² = {r2:.4f}', \n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaacfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for best model (typically Linear Regression)\n",
    "residuals = y_test_sal - y_pred_lr\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Residuals vs Predicted\n",
    "axes[0].scatter(y_pred_lr, residuals, alpha=0.6)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Salary', fontsize=11)\n",
    "axes[0].set_ylabel('Residuals', fontsize=11)\n",
    "axes[0].set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Histogram of residuals\n",
    "axes[1].hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residual Value', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Distribution of Residuals', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical test for normality\n",
    "statistic, p_value = stats.shapiro(residuals)\n",
    "print(f\"\\nShapiro-Wilk Test for Normality of Residuals:\")\n",
    "print(f\"  Statistic: {statistic:.6f}\")\n",
    "print(f\"  P-value: {p_value:.6f}\")\n",
    "if p_value > 0.05:\n",
    "    print(\"  ✓ Residuals appear normally distributed (p > 0.05)\")\n",
    "else:\n",
    "    print(\"  ⚠️  Residuals may not be normally distributed (p < 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f78132",
   "metadata": {},
   "source": [
    "## 2.8 Statistical Significance with Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9cf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use statsmodels for p-values and detailed statistics\n",
    "X_sal_with_const = sm.add_constant(X_train_sal_scaled)\n",
    "ols_model = sm.OLS(y_train_sal, X_sal_with_const)\n",
    "ols_result = ols_model.fit()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DETAILED STATISTICAL ANALYSIS (OLS Regression Summary)\")\n",
    "print(\"=\" * 70)\n",
    "print(ols_result.summary())\n",
    "\n",
    "# Extract coefficient table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COEFFICIENT SIGNIFICANCE\")\n",
    "print(\"=\" * 70)\n",
    "coef_table = pd.DataFrame({\n",
    "    'Feature': ['Intercept'] + available_perf_cols,\n",
    "    'Coefficient': ols_result.params.values,\n",
    "    'Std Error': ols_result.bse.values,\n",
    "    't-statistic': ols_result.tvalues.values,\n",
    "    'P-value': ols_result.pvalues.values,\n",
    "    'Significant': ['Yes' if p < 0.05 else 'No' for p in ols_result.pvalues.values]\n",
    "})\n",
    "print(coef_table.to_string(index=False))\n",
    "\n",
    "# Highlight significant predictors\n",
    "significant_features = coef_table[coef_table['P-value'] < 0.05]['Feature'].tolist()\n",
    "print(f\"\\n✓ Statistically significant predictors (p < 0.05):\")\n",
    "for feat in significant_features:\n",
    "    if feat != 'Intercept':\n",
    "        print(f\"  • {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598d04a",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 3: Age vs Performance - Linear and Polynomial Regression\n",
    "\n",
    "**Research Question**: How does age affect player performance?\n",
    "\n",
    "**Approach**: \n",
    "- Linear regression to test linear relationship\n",
    "- Polynomial regression to capture potential inverted-U pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da5ca3e",
   "metadata": {},
   "source": [
    "## 3.1 Load and Prepare Age-Performance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load player stats data\n",
    "age_df = pd.read_csv('../data_raw/playerStats.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", age_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(age_df.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(age_df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(age_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09af28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse age from \"YY-DDD\" format to just years\n",
    "def parse_age(age_str):\n",
    "    try:\n",
    "        if pd.isna(age_str):\n",
    "            return np.nan\n",
    "        # Extract years from format like \"25-064\"\n",
    "        return int(str(age_str).split('-')[0])\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "age_df['Age_Years'] = age_df['Age'].apply(parse_age)\n",
    "\n",
    "# Remove players with missing age\n",
    "age_df_clean = age_df.dropna(subset=['Age_Years']).copy()\n",
    "\n",
    "print(f\"Original dataset: {len(age_df)} players\")\n",
    "print(f\"After removing missing ages: {len(age_df_clean)} players\")\n",
    "print(f\"\\nAge distribution:\")\n",
    "print(age_df_clean['Age_Years'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert performance metrics to numeric\n",
    "performance_metrics = ['Gls', 'Ast', 'xG', 'npxG', 'xAG', 'Min', 'MP', 'PrgC', 'PrgP']\n",
    "\n",
    "# Check which columns exist\n",
    "available_metrics = [col for col in performance_metrics if col in age_df_clean.columns]\n",
    "print(f\"Available metrics: {available_metrics}\")\n",
    "\n",
    "# Convert to numeric\n",
    "for col in available_metrics:\n",
    "    age_df_clean[col] = pd.to_numeric(age_df_clean[col], errors='coerce')\n",
    "\n",
    "# Fill missing minutes with 0 (players who didn't play)\n",
    "if 'Min' in available_metrics:\n",
    "    age_df_clean['Min'] = age_df_clean['Min'].fillna(0)\n",
    "\n",
    "# Create per-90 metrics\n",
    "print(\"\\nCreating per-90 minute metrics...\")\n",
    "per90_metrics = []\n",
    "\n",
    "# Only create per-90 metrics for players with > 90 minutes\n",
    "age_df_clean = age_df_clean[age_df_clean['Min'] > 90].copy()\n",
    "\n",
    "for metric in ['Gls', 'Ast', 'xG', 'xAG', 'PrgC', 'PrgP']:\n",
    "    if metric in available_metrics:\n",
    "        new_col = f'{metric}_Per_90'\n",
    "        age_df_clean[new_col] = (age_df_clean[metric] / age_df_clean['Min']) * 90\n",
    "        per90_metrics.append(new_col)\n",
    "        print(f\"  Created: {new_col}\")\n",
    "\n",
    "# Remove any infinite values\n",
    "age_df_clean = age_df_clean.replace([np.inf, -np.inf], np.nan)\n",
    "age_df_clean = age_df_clean.dropna(subset=per90_metrics)\n",
    "\n",
    "print(f\"\\nFinal dataset for analysis: {len(age_df_clean)} players\")\n",
    "print(f\"Per-90 metrics created: {per90_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b23556",
   "metadata": {},
   "source": [
    "## 3.2 Create Composite Performance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a composite performance score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select key offensive metrics for composite score\n",
    "key_metrics = ['Gls_Per_90', 'Ast_Per_90', 'xG_Per_90']\n",
    "key_metrics = [m for m in key_metrics if m in age_df_clean.columns]\n",
    "\n",
    "if len(key_metrics) > 0:\n",
    "    # Scale each metric to 0-1\n",
    "    scaler_perf = MinMaxScaler()\n",
    "    scaled_metrics = scaler_perf.fit_transform(age_df_clean[key_metrics])\n",
    "    \n",
    "    # Composite = average of scaled metrics\n",
    "    age_df_clean['Performance_Score'] = scaled_metrics.mean(axis=1)\n",
    "    \n",
    "    print(\"Composite Performance Score created!\")\n",
    "    print(f\"Based on: {key_metrics}\")\n",
    "    print(f\"\\nPerformance Score statistics:\")\n",
    "    print(age_df_clean['Performance_Score'].describe())\n",
    "else:\n",
    "    print(\"⚠️  Not enough metrics to create composite score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f6f4a",
   "metadata": {},
   "source": [
    "## 3.3 Exploratory Analysis: Age vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aad7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationship between age and performance\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each per-90 metric vs age\n",
    "plot_metrics = per90_metrics[:5] + ['Performance_Score'] if 'Performance_Score' in age_df_clean.columns else per90_metrics[:6]\n",
    "\n",
    "for idx, metric in enumerate(plot_metrics):\n",
    "    if metric in age_df_clean.columns:\n",
    "        axes[idx].scatter(age_df_clean['Age_Years'], age_df_clean[metric], \n",
    "                         alpha=0.3, s=20)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(age_df_clean['Age_Years'], age_df_clean[metric], 2)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(age_df_clean['Age_Years'].min(), \n",
    "                            age_df_clean['Age_Years'].max(), 100)\n",
    "        axes[idx].plot(x_line, p(x_line), 'r-', linewidth=2, label='Polynomial Fit')\n",
    "        \n",
    "        axes[idx].set_xlabel('Age', fontsize=11)\n",
    "        axes[idx].set_ylabel(metric, fontsize=11)\n",
    "        axes[idx].set_title(f'Age vs {metric}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1acdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"CORRELATION BETWEEN AGE AND PERFORMANCE METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "correlations = []\n",
    "for metric in per90_metrics + (['Performance_Score'] if 'Performance_Score' in age_df_clean.columns else []):\n",
    "    if metric in age_df_clean.columns:\n",
    "        # Pearson correlation\n",
    "        pearson_r, pearson_p = stats.pearsonr(age_df_clean['Age_Years'], \n",
    "                                             age_df_clean[metric])\n",
    "        # Spearman correlation\n",
    "        spearman_r, spearman_p = stats.spearmanr(age_df_clean['Age_Years'], \n",
    "                                                age_df_clean[metric])\n",
    "        \n",
    "        correlations.append({\n",
    "            'Metric': metric,\n",
    "            'Pearson r': pearson_r,\n",
    "            'Pearson p': pearson_p,\n",
    "            'Spearman r': spearman_r,\n",
    "            'Spearman p': spearman_p,\n",
    "            'Significant': 'Yes' if pearson_p < 0.05 else 'No'\n",
    "        })\n",
    "\n",
    "corr_df = pd.DataFrame(correlations)\n",
    "print(corr_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n✓ Negative correlation = Performance decreases with age\")\n",
    "print(\"✓ Positive correlation = Performance increases with age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e95bb3",
   "metadata": {},
   "source": [
    "## 3.4 Linear Regression: Age vs Performance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "if 'Performance_Score' in age_df_clean.columns:\n",
    "    X_age = age_df_clean[['Age_Years']].values\n",
    "    y_perf = age_df_clean['Performance_Score'].values\n",
    "    \n",
    "    # Split data\n",
    "    X_train_age, X_test_age, y_train_perf, y_test_perf = train_test_split(\n",
    "        X_age, y_perf, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train linear model\n",
    "    lr_age_model = LinearRegression()\n",
    "    lr_age_model.fit(X_train_age, y_train_perf)\n",
    "    y_pred_age_lr = lr_age_model.predict(X_test_age)\n",
    "    \n",
    "    # Evaluate\n",
    "    r2_lr = r2_score(y_test_perf, y_pred_age_lr)\n",
    "    rmse_lr = np.sqrt(mean_squared_error(y_test_perf, y_pred_age_lr))\n",
    "    mae_lr = mean_absolute_error(y_test_perf, y_pred_age_lr)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"LINEAR REGRESSION: Age vs Performance Score\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Coefficient (slope): {lr_age_model.coef_[0]:.6f}\")\n",
    "    print(f\"Intercept: {lr_age_model.intercept_:.6f}\")\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"  R² Score:  {r2_lr:.4f}\")\n",
    "    print(f\"  RMSE:      {rmse_lr:.4f}\")\n",
    "    print(f\"  MAE:       {mae_lr:.4f}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    if lr_age_model.coef_[0] < 0:\n",
    "        print(f\"\\n✓ Negative slope: Performance decreases by {abs(lr_age_model.coef_[0]):.4f} per year of age\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Positive slope: Performance increases by {lr_age_model.coef_[0]:.4f} per year of age\")\n",
    "else:\n",
    "    print(\"Performance_Score not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057d2936",
   "metadata": {},
   "source": [
    "## 3.5 Polynomial Regression: Capturing Non-Linear Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1bf2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test polynomial degrees 2-4\n",
    "if 'Performance_Score' in age_df_clean.columns:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"POLYNOMIAL REGRESSION COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    poly_results = []\n",
    "    poly_models = {}\n",
    "    \n",
    "    for degree in [2, 3, 4]:\n",
    "        # Create polynomial features\n",
    "        poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        X_train_poly = poly.fit_transform(X_train_age)\n",
    "        X_test_poly = poly.transform(X_test_age)\n",
    "        \n",
    "        # Train model\n",
    "        poly_model = LinearRegression()\n",
    "        poly_model.fit(X_train_poly, y_train_perf)\n",
    "        y_pred_poly = poly_model.predict(X_test_poly)\n",
    "        \n",
    "        # Evaluate\n",
    "        r2 = r2_score(y_test_perf, y_pred_poly)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_perf, y_pred_poly))\n",
    "        mae = mean_absolute_error(y_test_perf, y_pred_poly)\n",
    "        \n",
    "        poly_results.append({\n",
    "            'Degree': degree,\n",
    "            'R²': r2,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae\n",
    "        })\n",
    "        \n",
    "        poly_models[degree] = (poly, poly_model)\n",
    "    \n",
    "    poly_df = pd.DataFrame(poly_results)\n",
    "    print(poly_df.to_string(index=False))\n",
    "    \n",
    "    # Best polynomial degree\n",
    "    best_degree = poly_df.loc[poly_df['R²'].idxmax(), 'Degree']\n",
    "    print(f\"\\n✓ Best polynomial degree: {int(best_degree)} (highest R²)\")\n",
    "else:\n",
    "    print(\"Performance_Score not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize linear vs polynomial fits\n",
    "if 'Performance_Score' in age_df_clean.columns:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Create smooth line for predictions\n",
    "    age_range = np.linspace(X_age.min(), X_age.max(), 100).reshape(-1, 1)\n",
    "    \n",
    "    # Plot 1: Linear regression\n",
    "    axes[0].scatter(X_test_age, y_test_perf, alpha=0.4, s=30, label='Actual')\n",
    "    axes[0].plot(age_range, lr_age_model.predict(age_range), \n",
    "                'r-', linewidth=2, label=f'Linear Fit (R²={r2_lr:.3f})')\n",
    "    axes[0].set_xlabel('Age', fontsize=12)\n",
    "    axes[0].set_ylabel('Performance Score', fontsize=12)\n",
    "    axes[0].set_title('Linear Regression', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Degree 2 polynomial\n",
    "    poly2, model2 = poly_models[2]\n",
    "    age_range_poly2 = poly2.transform(age_range)\n",
    "    r2_poly2 = poly_df[poly_df['Degree'] == 2]['R²'].values[0]\n",
    "    \n",
    "    axes[1].scatter(X_test_age, y_test_perf, alpha=0.4, s=30, label='Actual')\n",
    "    axes[1].plot(age_range, model2.predict(age_range_poly2), \n",
    "                'g-', linewidth=2, label=f'Quadratic Fit (R²={r2_poly2:.3f})')\n",
    "    axes[1].set_xlabel('Age', fontsize=12)\n",
    "    axes[1].set_ylabel('Performance Score', fontsize=12)\n",
    "    axes[1].set_title('Polynomial Regression (Degree 2)', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Best polynomial\n",
    "    best_poly, best_model = poly_models[int(best_degree)]\n",
    "    age_range_poly_best = best_poly.transform(age_range)\n",
    "    r2_best = poly_df[poly_df['Degree'] == best_degree]['R²'].values[0]\n",
    "    \n",
    "    axes[2].scatter(X_test_age, y_test_perf, alpha=0.4, s=30, label='Actual')\n",
    "    axes[2].plot(age_range, best_model.predict(age_range_poly_best), \n",
    "                'purple', linewidth=2, label=f'Degree {int(best_degree)} (R²={r2_best:.3f})')\n",
    "    axes[2].set_xlabel('Age', fontsize=12)\n",
    "    axes[2].set_ylabel('Performance Score', fontsize=12)\n",
    "    axes[2].set_title(f'Best Polynomial (Degree {int(best_degree)})', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962474f",
   "metadata": {},
   "source": [
    "## 3.6 Identify Peak Performance Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1439dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find age that maximizes predicted performance (for quadratic model)\n",
    "if 'Performance_Score' in age_df_clean.columns:\n",
    "    poly2, model2 = poly_models[2]\n",
    "    \n",
    "    # Generate predictions for all ages\n",
    "    all_ages = np.arange(16, 41).reshape(-1, 1)\n",
    "    all_ages_poly = poly2.transform(all_ages)\n",
    "    predictions = model2.predict(all_ages_poly)\n",
    "    \n",
    "    # Find peak\n",
    "    peak_idx = predictions.argmax()\n",
    "    peak_age = all_ages[peak_idx][0]\n",
    "    peak_performance = predictions[peak_idx]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"PEAK PERFORMANCE AGE ANALYSIS (Quadratic Model)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Peak Performance Age: {peak_age} years\")\n",
    "    print(f\"Predicted Performance at Peak: {peak_performance:.4f}\")\n",
    "    \n",
    "    # Performance at different life stages\n",
    "    ages_of_interest = [20, 25, 30, 35]\n",
    "    print(f\"\\nPredicted Performance by Age:\")\n",
    "    for age in ages_of_interest:\n",
    "        age_poly = poly2.transform([[age]])\n",
    "        perf = model2.predict(age_poly)[0]\n",
    "        pct_of_peak = (perf / peak_performance) * 100\n",
    "        print(f\"  Age {age}: {perf:.4f} ({pct_of_peak:.1f}% of peak)\")\n",
    "    \n",
    "    # Visualize peak\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_ages, predictions, 'b-', linewidth=2, label='Predicted Performance')\n",
    "    plt.scatter([peak_age], [peak_performance], s=200, c='red', \n",
    "               marker='*', zorder=5, label=f'Peak Age ({peak_age} years)')\n",
    "    plt.axvline(x=peak_age, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel('Age (years)', fontsize=12)\n",
    "    plt.ylabel('Predicted Performance Score', fontsize=12)\n",
    "    plt.title('Performance Trajectory Across Age', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e2601",
   "metadata": {},
   "source": [
    "## 3.7 Age Group Comparison (ANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db98a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age groups\n",
    "if 'Performance_Score' in age_df_clean.columns:\n",
    "    def categorize_age(age):\n",
    "        if age < 23:\n",
    "            return 'Young (<23)'\n",
    "        elif age <= 29:\n",
    "            return 'Prime (23-29)'\n",
    "        else:\n",
    "            return 'Experienced (30+)'\n",
    "    \n",
    "    age_df_clean['Age_Group'] = age_df_clean['Age_Years'].apply(categorize_age)\n",
    "    \n",
    "    # Group statistics\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PERFORMANCE BY AGE GROUP\")\n",
    "    print(\"=\" * 70)\n",
    "    group_stats = age_df_clean.groupby('Age_Group')['Performance_Score'].agg([\n",
    "        ('Count', 'count'),\n",
    "        ('Mean', 'mean'),\n",
    "        ('Std', 'std'),\n",
    "        ('Median', 'median'),\n",
    "        ('Min', 'min'),\n",
    "        ('Max', 'max')\n",
    "    ]).round(4)\n",
    "    print(group_stats)\n",
    "    \n",
    "    # ANOVA test\n",
    "    young = age_df_clean[age_df_clean['Age_Group'] == 'Young (<23)']['Performance_Score']\n",
    "    prime = age_df_clean[age_df_clean['Age_Group'] == 'Prime (23-29)']['Performance_Score']\n",
    "    experienced = age_df_clean[age_df_clean['Age_Group'] == 'Experienced (30+)']['Performance_Score']\n",
    "    \n",
    "    f_stat, p_value = stats.f_oneway(young, prime, experienced)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ANOVA TEST RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.6f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"\\n✓ SIGNIFICANT difference between age groups (p < 0.05)\")\n",
    "        print(\"  Age groups show statistically different performance levels\")\n",
    "    else:\n",
    "        print(\"\\n✗ NO significant difference between age groups (p >= 0.05)\")\n",
    "        print(\"  Cannot confidently conclude age groups differ in performance\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=age_df_clean, x='Age_Group', y='Performance_Score',\n",
    "               order=['Young (<23)', 'Prime (23-29)', 'Experienced (30+)'])\n",
    "    plt.title('Performance Distribution by Age Group', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Age Group', fontsize=12)\n",
    "    plt.ylabel('Performance Score', fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750d2fd2",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18849047",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \" * 20 + \"REGRESSION ANALYSIS - COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 1: POSSESSION VS MATCH OUTCOMES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Binary Classification (Win vs Not-Win):\")\n",
    "print(f\"  • Accuracy: {accuracy:.2%}\")\n",
    "print(f\"  • ROC-AUC: {roc_auc:.4f}\" if 'roc_auc' in locals() else \"  • ROC-AUC: N/A\")\n",
    "print(f\"  • Coefficient: {log_reg_binary.coef_[0][0]:.4f}\")\n",
    "print(f\"\\nMulticlass Classification (Win/Draw/Loss):\")\n",
    "print(f\"  • Accuracy: {accuracy_multi:.2%}\")\n",
    "print(f\"\\nConclusion: Possession shows {'WEAK' if accuracy < 0.60 else 'MODERATE' if accuracy < 0.75 else 'STRONG'} predictive power\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUESTION 2: SALARY VS PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "best_model_name = models_comparison.loc[models_comparison['R²'].idxmax(), 'Model']\n",
    "best_r2 = models_comparison['R²'].max()\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"  • R² Score: {best_r2:.4f} ({best_r2*100:.1f}% variance explained)\")\n",
    "print(f\"  • Performance explains {best_r2*100:.1f}% of salary variance\")\n",
    "print(f\"  • {100-best_r2*100:.1f}% of salary determined by other factors\")\n",
    "print(f\"\\nMost Important Features:\")\n",
    "top_features = abs_coefs.head(3)\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"  • {row['Feature']}: {row['Avg_Abs_Coef']:.4f}\")\n",
    "\n",
    "if 'Performance_Score' in age_df_clean.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUESTION 3: AGE VS PERFORMANCE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Linear Regression:\")\n",
    "    print(f\"  • R² Score: {r2_lr:.4f}\")\n",
    "    print(f\"  • Coefficient: {lr_age_model.coef_[0]:.6f} (performance/year)\")\n",
    "    print(f\"\\nBest Polynomial Model (Degree {int(best_degree)}):\")\n",
    "    print(f\"  • R² Score: {r2_best:.4f}\")\n",
    "    print(f\"  • Peak Performance Age: {peak_age} years\")\n",
    "    print(f\"\\nAge Group Differences:\")\n",
    "    print(f\"  • ANOVA p-value: {p_value:.6f}\")\n",
    "    print(f\"  • Result: {'SIGNIFICANT' if p_value < 0.05 else 'NOT SIGNIFICANT'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Possession has limited predictive power for match outcomes\")\n",
    "print(\"2. Performance metrics explain only a portion of player salaries\")\n",
    "print(\"3. Age shows a non-linear relationship with performance (inverted-U)\")\n",
    "print(\"4. Multiple factors beyond simple metrics influence football success\")\n",
    "print(\"\\nRecommendation: Use regression models as ONE tool in a comprehensive\")\n",
    "print(\"                analysis framework, not as the sole decision-making basis\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
